DESAFIO BIG DATA/BI

üìå O QUE FOI FEITO?
Neste desafio foram feitas as ingest√µes dos dados que est√£o na pasta /raw onde vamos ter alguns arquivos .csv de um banco relacional de vendas.

 - VENDAS.CSV
 - CLIENTES.CSV
 - ENDERECO.CSV
 - REGIAO.CSV
 - DIVISAO.CSV

O Objetivo do trabalho foi atingido, que era basicamente prover dados em uma pasta desafio_curso/gold em .csv para ser consumido por um relat√≥rio em PowerBI dentro da pasta 'app'

üìë ETAPAS DO DESAFIO

Etapa 1 - Envio dos arquivos para o HDFS
     Foi criado um shell script dentro da pasta input/scripts/pre_process que automatiza a cria√ß√£o do ambiente no HDFS, assim criando as pastas datalake e raw.
     Ap√≥s isso o c√≥digo copia os CSVs para o ambiente do HDFS em pastas com o mesmo nome do arquivo em si.

Etapa 2 - Cria√ß√£o do banco DESAFIO_CURSO dentro do Hive Server Criar o banco DEASFIO_CURSO e dentro tabelas no Hive usando o HQL e executando um script shell dentro do hive server na pasta scripts/pre_process.
     Nesta etapa foi apenas executado o shell script criado na etapa anterior, que cria o database desafio_curso dentro do Hive Server e as tabelas baseadas nos CSVs, seguindo essa estrtutura abaixo:
     
    - DESAFIO_CURSO 
        - TBL_VENDAS
        - TBL_CLIENTES
        - TBL_ENDERECO
        - TBL_REGIAO
        - TBL_DIVISAO
    As tabelas foram criadas a partir do shell script, que dentro de seu loop utiliza os arquivos HQL presentes na pasta input/scripts/hql 

Etapa 3 - Processamento dos dados no Spark
     Utilizando o spark, os dados foram tratados e processados, dentro de dataframes que buscam os dados diretamente do Hive Server.
     Os dados de todas as tabelas foram armazenados em um √∫nico dataframe com o nome df_stage, para que fosse facilitada a cria√ß√£o da tabela fato e suas dimens√µes.
     Todo o c√≥digo deste processo descrito est√° armazenado no arquivo process.py dentro da pasta input/scripts/process e l√° contendo mais detalhes do processo realizado junto a seus c√≥digos

Etapa 4 - Armazenamento das informa√ß√µes na tabela fato e suas dimens√µes
    
   Como descrito na etapa anterior, os dados trataram foram colocados na fato vendas e suas dimens√µes(Clientes, Tempo e Local) como √© poss√≠vel ver na estrutura abaixo:

        - FT_VENDAS
        - DIM_CLIENTES
        - DIM_TEMPO
        - DIM_LOCALIDADE

Etapa 5 - Exporta√ß√£o dos dados para a pasta input/gold
     A exporta√ß√£o dos dados presentes na tabela fato e suas dimens√µes foram exportados para arquivos CSVs para a pasta input/gold, c√≥digo tamb√©m presente no arquivo process.py

√öltima etapa - Cria√ß√£o de gr√°ficos no Power BI a partir dos dados salvos na pasta Gold. Estes gr√°ficos est√£o salvos no arquivo Proj_Vendas.pbix na pasta input/app
