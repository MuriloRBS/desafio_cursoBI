<h1 align="center">DESAFIO BIG DATA/BI</h3>

## Tecnologias e Linguagens Utilizadas
<p align="left"> <a href="https://www.docker.com/" target="_blank" rel="noreferrer"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/docker/docker-original-wordmark.svg" alt="docker" width="40" height="40"/> </a> <a href="https://git-scm.com/" target="_blank" rel="noreferrer"> <img src="https://www.vectorlogo.zone/logos/git-scm/git-scm-icon.svg" alt="git" width="40" height="40"/> </a> <a href="https://hadoop.apache.org/" target="_blank" rel="noreferrer"> <img src="https://www.vectorlogo.zone/logos/apache_hadoop/apache_hadoop-icon.svg" alt="hadoop" width="40" height="40"/> </a> <a href="https://hive.apache.org/" target="_blank" rel="noreferrer"> <img src="https://www.vectorlogo.zone/logos/apache_hive/apache_hive-icon.svg" alt="hive" width="40" height="40"/> </a> <a href="https://www.python.org" target="_blank" rel="noreferrer"> <img src="https://raw.githubusercontent.com/devicons/devicon/master/icons/python/python-original.svg" alt="python" width="40" height="40"/> </a> </p>

## üìå QUAL FOI A PROPOSTA DO DESAFIO?
Foi proposto neste desafio uma pipeline de ingest√£o de dados, utilizando Hive, em um banco de dados, com esses dados esses vindos inicialmente dos seguintes arquivos CSVs:

 - VENDAS.CSV
 - CLIENTES.CSV
 - ENDERECO.CSV
 - REGIAO.CSV
 - DIVISAO.CSV

## :bulb: O PROCESSO
### O processo √© resumidamente √© tratar os dados citados acima e transform√°-los em tabelas dimensionais, assim facilitando sua utiliza√ß√£o em gr√°ficos no Power BI



## üìë ETAPAS DETALHADAS DO DESAFIO

**Etapa 1** - Envio dos arquivos para o HDFS
     Foi criado um shell script dentro da pasta input/scripts/pre_process que automatiza a cria√ß√£o do ambiente no HDFS, assim criando as pastas datalake e raw.
     Ap√≥s isso o c√≥digo copia os CSVs para o ambiente do HDFS em pastas com o mesmo nome do arquivo em si.

**Etapa 2** - Cria√ß√£o do banco DESAFIO_CURSO dentro do Hive Server Criar o banco DEASFIO_CURSO e dentro tabelas no Hive usando o HQL e executando um script shell dentro do hive server na pasta scripts/pre_process.
     Nesta etapa foi apenas executado o shell script criado na etapa anterior, que cria o database desafio_curso dentro do Hive Server e as tabelas baseadas nos CSVs, seguindo essa estrtutura abaixo:
     
    - DESAFIO_CURSO 
        - TBL_VENDAS
        - TBL_CLIENTES
        - TBL_ENDERECO
        - TBL_REGIAO
        - TBL_DIVISAO  
   As tabelas foram criadas a partir do shell script, que dentro de seu loop utiliza os arquivos HQL presentes na pasta input/scripts/hql 

**Etapa 3** - Processamento dos dados no Spark
     Utilizando o spark, os dados foram tratados e processados, dentro de dataframes que buscam os dados diretamente do Hive Server.
     Os dados de todas as tabelas foram armazenados em um √∫nico dataframe com o nome df_stage, para que fosse facilitada a cria√ß√£o da tabela fato e suas dimens√µes.
     Todo o c√≥digo deste processo descrito est√° armazenado no arquivo process.py dentro da pasta input/scripts/process e l√° contendo mais detalhes do processo realizado junto a seus c√≥digos

**Etapa 4** - Armazenamento das informa√ß√µes na tabela fato e suas dimens√µes
    
   Como descrito na etapa anterior, os dados tratados foram colocados na fato vendas e suas dimens√µes(Clientes, Tempo e Local) como √© poss√≠vel ver na estrutura abaixo:

        - FT_VENDAS
        - DIM_CLIENTES
        - DIM_TEMPO
        - DIM_LOCALIDADE

**Etapa 5** - Exporta√ß√£o dos dados para a pasta input/gold
     A exporta√ß√£o dos dados presentes na tabela fato e suas dimens√µes foram exportados para arquivos CSVs para a pasta input/gold, c√≥digo tamb√©m presente no arquivo process.py

**Etapa 6** - Cria√ß√£o de gr√°ficos no Power BI a partir dos dados salvos na pasta Gold. Estes gr√°ficos est√£o salvos no arquivo Proj_Vendas.pbix na pasta input/app
